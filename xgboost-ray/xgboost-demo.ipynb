{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87906699",
   "metadata": {},
   "source": [
    "# Scaling ML workflows with Ray\n",
    "\n",
    "We are going to scale an end to end machine learning workload:\n",
    "\n",
    "1. Data loading\n",
    "2. Training\n",
    "2. Hyperparameter tuning\n",
    "3. Inference\n",
    "\n",
    "First, we try on a local node with a data set, time it, and then try on an Anyscale cluster with multiple nodes and multiple cores.\n",
    "\n",
    "We should observe noticeable difference.\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/ETLioF3e6PLPYr5DggvUl/7f8247e1bf79ab49295882259bd5420d/localMachineCloud.png\" width=\"40%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e869dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import xgboost as xgb\n",
    "import xgboost_ray\n",
    "from xgboost.callback import TrainingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c044fb",
   "metadata": {},
   "source": [
    "## Read the data files into various chunks\n",
    "\n",
    "Read the classifation data features and labes into 300MB, 3GB, and 11GB chunks to\n",
    "illustrate training, tunning, and inferencing at scale.\n",
    "\n",
    "The data is generated from `sklearn.datasets make_classification` with 2 classes as labels as default. More rows or classes can be easily generated. For this demo, we\n",
    "will use the following: \n",
    "\n",
    " * 10,000,000 rows\n",
    " * 40 feature columns\n",
    " * 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57229d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_parquet_files(path, size=10):\n",
    "    \"\"\"Get all parquet parts from a directory.\"\"\"\n",
    "    size *= 10\n",
    "    files = sorted(glob.glob(path))\n",
    "    while size > len(files):\n",
    "        files = files + files\n",
    "    files = files[0:size]\n",
    "    return files\n",
    "\n",
    "def load_parquet_dataset(files):\n",
    "    \"\"\"Load all parquet files into a pandas df.\"\"\"\n",
    "    df = pd.read_parquet(files[0])\n",
    "    for i in tqdm.tqdm(range(1, len(files), 50)):\n",
    "        df = pd.concat((df, pd.read_parquet(files[i:i+50])))\n",
    "        memory_usage = df.memory_usage(deep=True).sum()/1e9\n",
    "        tqdm.tqdm.write(f\"Dataset size: {memory_usage} GB\")\n",
    "        if memory_usage > 12:\n",
    "            raise MemoryError(f\"Dataset too big to fit into memory!\")\n",
    "    return df[sorted(df.columns)].drop(\"partition\", axis=1)\n",
    "\n",
    "class TqdmCallback(TrainingCallback):\n",
    "    \"\"\"Simple callback to print a progress bar\"\"\"\n",
    "    def __init__(self, num_samples: int) -> None:\n",
    "        self.num_samples = num_samples\n",
    "        super().__init__()\n",
    "\n",
    "    def before_training(self, model):\n",
    "        if xgb.rabit.get_rank() == 0:\n",
    "            self.pbar = tqdm.tqdm(total=self.num_samples)\n",
    "        return model\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if xgb.rabit.get_rank() == 0:\n",
    "            self.pbar.update(1)\n",
    "\n",
    "    def after_training(self, model):\n",
    "        if xgb.rabit.get_rank() == 0:\n",
    "            self.pbar.close()\n",
    "        return model\n",
    "\n",
    "data_path = f\"/home/ec2-user/data/classification.parquet/**/*.parquet\"\n",
    "\n",
    "data_files_300MB = get_parquet_files(data_path, size=1)\n",
    "data_files_3GB = get_parquet_files(data_path, size=10)\n",
    "data_files_11GB = get_parquet_files(data_path, size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa2b5b",
   "metadata": {},
   "source": [
    "# 1a. Training (regular XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b69b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost config.\n",
    "xgboost_params = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "\n",
    "from xgboost import DMatrix, train\n",
    "\n",
    "def train_xgboost(config, files, progress_bar=True):\n",
    "    start_time = time.time()\n",
    "\n",
    "    target_column = \"labels\"\n",
    "\n",
    "    # local loading of a parquet dataset\n",
    "    # needs conversion to pandas as vanilla\n",
    "    # xgboost can't work with parquet directly\n",
    "    train_df = load_parquet_dataset(files[:int(len(files) * 0.75)])\n",
    "    test_df = load_parquet_dataset(files[int(len(files) * 0.75):])\n",
    "    train_x = train_df.drop(target_column, axis=1)\n",
    "    train_y = train_df[target_column]\n",
    "    test_x = test_df.drop(target_column, axis=1)\n",
    "    test_y = test_df[target_column]\n",
    "\n",
    "    train_set = DMatrix(train_x, train_y)\n",
    "    test_set = DMatrix(test_x, test_y)\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    # Train the classifier\n",
    "    bst = train(params=config,\n",
    "                dtrain=train_set,\n",
    "                evals=[(test_set, \"eval\")],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=False,\n",
    "                num_boost_round=10,\n",
    "                callbacks=[TqdmCallback(10)] if progress_bar else [])\n",
    "    print(f\"Total time taken: {time.time()-start_time}\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60746444",
   "metadata": {},
   "source": [
    "Try a smaller data set (300MB) to ensure the our xgboost trainer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abce339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 0.23590022 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 0.101100124 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 18.82234835624695\n",
      "Final validation error: 0.1478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bst = train_xgboost(xgboost_params, data_files_300MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfd9df",
   "metadata": {},
   "source": [
    "Try a larger data set (3GB) to ensure the our xgboost trainer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff178286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:13<00:13, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1.718702504 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2.7896 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 0.842500748 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:41<00:00, 22.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 254.95771074295044\n",
      "Final validation error: 0.1472\n"
     ]
    }
   ],
   "source": [
    "bst = train_xgboost(xgboost_params, data_files_3GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff728868",
   "metadata": {},
   "source": [
    "# 1b. Training (XGBoost on Ray)\n",
    "\n",
    "Requires only few lines of code changes to XGBoost to use distributed training on Ray:\n",
    "\n",
    " * `from xgboost_ray import RayDMatrix, train, RayParams`\n",
    " * Use `RayDMatrix` distributed [sharding and reading](https://github.com/ray-project/xgboost_ray#distributed-data-loading) to convert to XGBoost internal data structure\n",
    " * Add additional `RayParams` argument to `train_xgboost(...)` for level of parallelism\n",
    " \n",
    " This diagram depicts how Distributed XGBoost-Ray works on a Ray cluster\n",
    " \n",
    " <img src=\"images/xgboost_distributed.png\" width=\"40%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df0390",
   "metadata": {},
   "source": [
    "Let's start Ray on the local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077c7433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.0.0.231',\n",
       " 'raylet_ip_address': '10.0.0.231',\n",
       " 'redis_address': '10.0.0.231:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-01-26_19-15-54_415773_15641/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-01-26_19-15-54_415773_15641/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-01-26_19-15-54_415773_15641',\n",
       " 'metrics_export_port': 63950,\n",
       " 'node_id': '7e2bd7cfb7e67522dcadb22a0c0e8b55d02f0a3e3ae71f32ef5de8c9'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73401ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost config.\n",
    "xgboost_params = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "\n",
    "from xgboost_ray import RayDMatrix, train, RayParams\n",
    "\n",
    "def train_xgboost_remote(config, files, ray_params, progress_bar=True):\n",
    "    start_time = time.time()\n",
    "\n",
    "    target_column = \"labels\"\n",
    "    ds = ray.data.read_parquet(files)\n",
    "    ds.repartition(32)\n",
    "\n",
    "    split_index = int(ds.count() * (1 -0.3))\n",
    "    X = ds.random_shuffle()\n",
    "    X_train, X_valid = X.split_at_indices([split_index]) \n",
    "\n",
    "    ds_train = X_train.repartition(8)\n",
    "    ds_valid = X_valid.repartition(8)\n",
    "    train_set = RayDMatrix(ds_train, target_column, ignore=[\"partition\"])\n",
    "    test_set = RayDMatrix(ds_valid, target_column, ignore=[\"partition\"])\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    # Train the classifier\n",
    "    bst = train(params=config,\n",
    "                dtrain=train_set,\n",
    "                evals=[(test_set, \"eval\")],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=False,\n",
    "                num_boost_round=10,                       # equivalent to epochs or iterations\n",
    "                callbacks=[TqdmCallback(10)] if progress_bar else [],\n",
    "                ray_params=ray_params)                    # Ray parameters for parallelism\n",
    "    print(f\"Total time taken: {time.time()-start_time}\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))\n",
    "\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7a95a",
   "metadata": {},
   "source": [
    "#### Make dataset available in cloud object storage\n",
    " * We uploaded the dataset in Cloud storage (S3) and changed the URL for each file in our 3 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743893cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_files_300MB = [str(i).replace('/home/ec2-user/', 's3://anyscale-demo/') for i in data_files_300MB]\n",
    "s3_data_files_3GB = [str(i).replace('/home/ec2-user/', 's3://anyscale-demo/') for i in data_files_3GB]\n",
    "s3_data_files_11GB = [str(i).replace('/home/ec2-user/', 's3://anyscale-demo/') for i in data_files_11GB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e692ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s3://anyscale-demo/data/classification.parquet/partition=0/part_0.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=1/part_1.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=10/part_10.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=11/part_11.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=12/part_12.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=13/part_13.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=14/part_14.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=15/part_15.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=16/part_16.parquet',\n",
       " 's3://anyscale-demo/data/classification.parquet/partition=17/part_17.parquet']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_files_300MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7a95a",
   "metadata": {},
   "source": [
    "#### Define RayParams for XGBoost level of parallelism\n",
    " * Eight actors, \n",
    " * Each using 2-4 CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9602e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:  31%|███▏      | 10/32 [00:00<00:00, 99.20it/s]\n",
      "Repartition:  88%|████████▊ | 28/32 [00:00<00:00, 143.14it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:01<00:00, 16.20it/s] \n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:01,  5.03it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 28.59it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 72.52it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 124.54it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 239.14it/s]\n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=15864)\u001b[0m 2022-01-26 19:18:28,931\tINFO main.py:979 -- [RayXGBoost] Created 4 new actors (4 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=15864)\u001b[0m 2022-01-26 19:18:31,033\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=15861)\u001b[0m [19:18:31] task [xgboost.ray]:140666353827072 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=15865)\u001b[0m [19:18:31] task [xgboost.ray]:140427781286496 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=15860)\u001b[0m [19:18:31] task [xgboost.ray]:140500780257344 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=16120)\u001b[0m [19:18:31] task [xgboost.ray]:140386172489344 got new rank 0\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]6120)\u001b[0m \n",
      " 10%|█         | 1/10 [00:02<00:22,  2.54s/it]m \n",
      " 20%|██        | 2/10 [00:04<00:15,  1.92s/it]m \n",
      " 30%|███       | 3/10 [00:05<00:12,  1.75s/it]m \n",
      " 40%|████      | 4/10 [00:06<00:09,  1.56s/it]m \n",
      " 50%|█████     | 5/10 [00:08<00:07,  1.46s/it]m \n",
      " 60%|██████    | 6/10 [00:09<00:05,  1.40s/it]m \n",
      " 70%|███████   | 7/10 [00:10<00:04,  1.40s/it]m \n",
      " 80%|████████  | 8/10 [00:12<00:02,  1.35s/it]m \n",
      " 90%|█████████ | 9/10 [00:13<00:01,  1.32s/it]m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=15864)\u001b[0m Total time taken: 47.01284575462341\n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=15864)\u001b[0m Final validation error: 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it] \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=15864)\u001b[0m 2022-01-26 19:18:46,298\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 17.43 seconds (15.26 pure XGBoost training time).\n"
     ]
    }
   ],
   "source": [
    "remote_train_xgboost_remote = ray.remote(train_xgboost_remote)\n",
    "bst = ray.get(remote_train_xgboost_remote.remote(xgboost_params, s3_data_files_300MB, RayParams(num_actors=4, cpus_per_actor=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf878073",
   "metadata": {},
   "source": [
    "Let's now point our training job jto an anyscale cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fad943b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mAuthenticating\u001b[0m\n",
      "Loaded Anyscale authentication token from ~/.anyscale/credentials.json.\n",
      "\n",
      "\u001b[1m\u001b[36mOutput\u001b[0m\n",
      "\u001b[22m\u001b[33m(anyscale +1.1s)\u001b[0m WARNING: No working_dir specified! Files will only be uploaded to the cluster if a working_dir is provided or a project is detected. In the future, files will only be uploaded if working_dir is provided. To ensure files continue being imported going forward, set the working_dir in your runtime environment. See https://docs.ray.io/en/latest/handling-dependencies.html#runtime-environments.\n",
      "\u001b[1m\u001b[36m(anyscale +0.7s)\u001b[0m .anyscale.yaml found in project_dir. Directory is attached to a project.\n",
      "\u001b[1m\u001b[36m(anyscale +0.9s)\u001b[0m Using project (name: phi-demos, project_dir: /home/ec2-user/working_dir, id: prj_26wrN4CCEw3fGhXMxzRAwvuk).\n",
      "\u001b[1m\u001b[36m(anyscale +2.0s)\u001b[0m cluster xgboost-demo2 is currently running, the cluster will not be restarted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 19:19:24,408\tINFO packaging.py:352 -- Creating a file package for local directory '/home/ec2-user/working_dir'.\n",
      "2022-01-26 19:19:24,427\tINFO packaging.py:221 -- Pushing file package 'gcs://_ray_pkg_7954c209209536de.zip' (0.78MiB) to Ray cluster...\n",
      "2022-01-26 19:19:24,899\tINFO packaging.py:224 -- Successfully pushed file package 'gcs://_ray_pkg_7954c209209536de.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m(anyscale +14.1s)\u001b[0m Connected to xgboost-demo2, see: https://console.anyscale.com/projects/prj_26wrN4CCEw3fGhXMxzRAwvuk/clusters/ses_aqKYRsRQpAUFD4gcfPrxnZ8e\n",
      "\u001b[1m\u001b[36m(anyscale +14.1s)\u001b[0m URL for head node of cluster: https://session-aqkyrsrqpaufd4gcfprxnz8e.i.anyscaleuserdata.com\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnyscaleClientContext(dashboard_url='https://session-aqkyrsrqpaufd4gcfprxnz8e.i.anyscaleuserdata.com/auth/?token=af19b5ad-165f-4d57-a596-3bd9d882b96e&redirect_to=dashboard', python_version='3.8.5', ray_version='1.9.1', ray_commit='2cdbf974ea63caf4323aacbccaef2394a14a8562', protocol_version='2021-09-22', _num_clients=1, _context_to_restore=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(\"anyscale://xgboost-demo2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21218816",
   "metadata": {},
   "source": [
    "We can now run our training job using our 11GB training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "113275b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata Fetch Progress:   0%|          | 0/50 [00:00<?, ?it/s]\n",
      "Metadata Fetch Progress:   2%|▏         | 1/50 [00:05<04:48,  5.89s/it]\n",
      "Metadata Fetch Progress:   4%|▍         | 2/50 [00:06<02:25,  3.03s/it]\n",
      "Metadata Fetch Progress:   6%|▌         | 3/50 [00:08<01:41,  2.16s/it]\n",
      "Metadata Fetch Progress:   8%|▊         | 4/50 [00:09<01:22,  1.80s/it]\n",
      "Metadata Fetch Progress:  10%|█         | 5/50 [00:09<00:57,  1.27s/it]\n",
      "Metadata Fetch Progress:  12%|█▏        | 6/50 [00:10<00:49,  1.13s/it]\n",
      "Metadata Fetch Progress:  14%|█▍        | 7/50 [00:10<00:34,  1.23it/s]\n",
      "Metadata Fetch Progress:  16%|█▌        | 8/50 [00:11<00:38,  1.09it/s]\n",
      "Metadata Fetch Progress:  18%|█▊        | 9/50 [00:11<00:27,  1.50it/s]\n",
      "Metadata Fetch Progress:  20%|██        | 10/50 [00:12<00:30,  1.29it/s]\n",
      "Metadata Fetch Progress:  22%|██▏       | 11/50 [00:13<00:22,  1.70it/s]\n",
      "Metadata Fetch Progress:  26%|██▌       | 13/50 [00:14<00:19,  1.87it/s]\n",
      "Metadata Fetch Progress:  28%|██▊       | 14/50 [00:14<00:17,  2.11it/s]\n",
      "Metadata Fetch Progress:  32%|███▏      | 16/50 [00:15<00:14,  2.33it/s]\n",
      "Metadata Fetch Progress:  34%|███▍      | 17/50 [00:15<00:14,  2.34it/s]\n",
      "Metadata Fetch Progress:  38%|███▊      | 19/50 [00:16<00:14,  2.13it/s]\n",
      "Metadata Fetch Progress:  42%|████▏     | 21/50 [00:16<00:09,  2.93it/s]\n",
      "Metadata Fetch Progress:  46%|████▌     | 23/50 [00:17<00:10,  2.58it/s]\n",
      "Metadata Fetch Progress:  48%|████▊     | 24/50 [00:17<00:08,  2.97it/s]\n",
      "Metadata Fetch Progress:  52%|█████▏    | 26/50 [00:17<00:05,  4.11it/s]\n",
      "Metadata Fetch Progress:  54%|█████▍    | 27/50 [00:18<00:08,  2.82it/s]\n",
      "Metadata Fetch Progress:  56%|█████▌    | 28/50 [00:19<00:07,  3.04it/s]\n",
      "Metadata Fetch Progress:  62%|██████▏   | 31/50 [00:20<00:06,  2.99it/s]\n",
      "Metadata Fetch Progress:  64%|██████▍   | 32/50 [00:20<00:05,  3.20it/s]\n",
      "Metadata Fetch Progress:  66%|██████▌   | 33/50 [00:20<00:04,  3.73it/s]\n",
      "Metadata Fetch Progress:  72%|███████▏  | 36/50 [00:21<00:04,  3.32it/s]\n",
      "Metadata Fetch Progress:  74%|███████▍  | 37/50 [00:21<00:03,  3.70it/s]\n",
      "Metadata Fetch Progress:  80%|████████  | 40/50 [00:21<00:01,  5.67it/s]\n",
      "Metadata Fetch Progress:  82%|████████▏ | 41/50 [00:22<00:02,  3.63it/s]\n",
      "Metadata Fetch Progress:  84%|████████▍ | 42/50 [00:22<00:01,  4.15it/s]\n",
      "Metadata Fetch Progress:  86%|████████▌ | 43/50 [00:22<00:01,  4.74it/s]\n",
      "Metadata Fetch Progress:  92%|█████████▏| 46/50 [00:23<00:00,  4.07it/s]\n",
      "Metadata Fetch Progress:  94%|█████████▍| 47/50 [00:23<00:00,  3.77it/s]\n",
      "Metadata Fetch Progress:  98%|█████████▊| 49/50 [00:23<00:00,  5.18it/s]\n",
      "Metadata Fetch Progress: 100%|██████████| 50/50 [00:25<00:00,  1.98it/s]\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:   3%|▎         | 1/32 [00:00<00:08,  3.51it/s]\n",
      "Repartition:   9%|▉         | 3/32 [00:01<00:10,  2.78it/s]\n",
      "Repartition:  16%|█▌        | 5/32 [00:01<00:05,  4.80it/s]\n",
      "Repartition:  19%|█▉        | 6/32 [00:01<00:09,  2.87it/s]\n",
      "Repartition:  25%|██▌       | 8/32 [00:02<00:06,  3.84it/s]\n",
      "Repartition:  28%|██▊       | 9/32 [00:02<00:05,  4.10it/s]\n",
      "Repartition:  41%|████      | 13/32 [00:02<00:02,  8.20it/s]\n",
      "Repartition:  53%|█████▎    | 17/32 [00:02<00:01, 11.03it/s]\n",
      "Repartition:  62%|██████▎   | 20/32 [00:02<00:01, 11.53it/s]\n",
      "Repartition:  78%|███████▊  | 25/32 [00:03<00:00, 15.39it/s]\n",
      "Repartition:  94%|█████████▍| 30/32 [00:04<00:00,  7.08it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:04<00:00,  6.75it/s]\n",
      "Shuffle Map:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Shuffle Map:   0%|          | 1/200 [00:01<03:35,  1.08s/it]\n",
      "Shuffle Map:   3%|▎         | 6/200 [00:01<00:30,  6.38it/s]\n",
      "Shuffle Map:   4%|▍         | 9/200 [00:01<00:20,  9.18it/s]\n",
      "Shuffle Map:   6%|▌         | 12/200 [00:01<00:16, 11.34it/s]\n",
      "Shuffle Map:  10%|█         | 21/200 [00:01<00:07, 24.71it/s]\n",
      "Shuffle Map:  15%|█▌        | 30/200 [00:01<00:04, 37.15it/s]\n",
      "Shuffle Map:  18%|█▊        | 36/200 [00:01<00:03, 41.63it/s]\n",
      "Shuffle Map:  21%|██        | 42/200 [00:01<00:03, 42.22it/s]\n",
      "Shuffle Map:  24%|██▍       | 48/200 [00:02<00:04, 35.67it/s]\n",
      "Shuffle Map:  26%|██▋       | 53/200 [00:02<00:05, 26.91it/s]\n",
      "Shuffle Map:  28%|██▊       | 57/200 [00:02<00:06, 22.30it/s]\n",
      "Shuffle Map:  30%|███       | 60/200 [00:02<00:06, 22.09it/s]\n",
      "Shuffle Map:  32%|███▏      | 63/200 [00:03<00:06, 21.55it/s]\n",
      "Shuffle Map:  33%|███▎      | 66/200 [00:03<00:06, 20.67it/s]\n",
      "Shuffle Map:  35%|███▌      | 70/200 [00:03<00:05, 23.57it/s]\n",
      "Shuffle Map:  36%|███▋      | 73/200 [00:03<00:05, 23.38it/s]\n",
      "Shuffle Map:  38%|███▊      | 77/200 [00:03<00:04, 26.93it/s]\n",
      "Shuffle Map:  42%|████▏     | 83/200 [00:03<00:03, 31.48it/s]\n",
      "Shuffle Map:  46%|████▌     | 92/200 [00:03<00:02, 43.19it/s]\n",
      "Shuffle Map:  51%|█████     | 102/200 [00:03<00:01, 56.34it/s]\n",
      "Shuffle Map:  55%|█████▍    | 109/200 [00:04<00:02, 34.41it/s]\n",
      "Shuffle Map:  57%|█████▊    | 115/200 [00:04<00:02, 34.63it/s]\n",
      "Shuffle Map:  61%|██████    | 122/200 [00:04<00:01, 39.73it/s]\n",
      "Shuffle Map:  64%|██████▎   | 127/200 [00:04<00:02, 36.09it/s]\n",
      "Shuffle Map:  66%|██████▌   | 132/200 [00:04<00:01, 37.43it/s]\n",
      "Shuffle Map:  70%|███████   | 140/200 [00:05<00:01, 45.63it/s]\n",
      "Shuffle Map:  73%|███████▎  | 146/200 [00:05<00:01, 40.82it/s]\n",
      "Shuffle Map:  76%|███████▌  | 152/200 [00:05<00:01, 44.72it/s]\n",
      "Shuffle Map:  80%|████████  | 161/200 [00:05<00:00, 51.17it/s]\n",
      "Shuffle Map:  84%|████████▎ | 167/200 [00:05<00:00, 52.06it/s]\n",
      "Shuffle Map:  88%|████████▊ | 175/200 [00:05<00:00, 57.72it/s]\n",
      "Shuffle Map:  91%|█████████ | 182/200 [00:06<00:00, 37.44it/s]\n",
      "Shuffle Map:  94%|█████████▎| 187/200 [00:06<00:00, 33.99it/s]\n",
      "Shuffle Map:  97%|█████████▋| 194/200 [00:06<00:00, 39.33it/s]\n",
      "Shuffle Map: 100%|██████████| 200/200 [00:06<00:00, 30.39it/s]\n",
      "Shuffle Reduce:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Shuffle Reduce:   0%|          | 1/200 [00:01<04:36,  1.39s/it]\n",
      "Shuffle Reduce:   1%|          | 2/200 [00:01<02:06,  1.56it/s]\n",
      "Shuffle Reduce:   2%|▏         | 4/200 [00:01<01:10,  2.79it/s]\n",
      "Shuffle Reduce:   4%|▎         | 7/200 [00:03<01:45,  1.84it/s]\n",
      "Shuffle Reduce:   4%|▍         | 9/200 [00:04<01:13,  2.60it/s]\n",
      "Shuffle Reduce:   5%|▌         | 10/200 [00:04<01:10,  2.70it/s]\n",
      "Shuffle Reduce:   7%|▋         | 14/200 [00:04<00:34,  5.32it/s]\n",
      "Shuffle Reduce:  14%|█▍        | 28/200 [00:04<00:09, 17.71it/s]\n",
      "Shuffle Reduce:  24%|██▎       | 47/200 [00:04<00:04, 37.66it/s]\n",
      "Shuffle Reduce:  40%|████      | 81/200 [00:04<00:01, 80.48it/s]\n",
      "Shuffle Reduce:  50%|████▉     | 99/200 [00:04<00:01, 97.16it/s]\n",
      "Shuffle Reduce:  61%|██████    | 122/200 [00:05<00:00, 121.35it/s]\n",
      "Shuffle Reduce:  70%|███████   | 141/200 [00:06<00:01, 45.16it/s] \n",
      "Shuffle Reduce:  78%|███████▊  | 155/200 [00:07<00:01, 27.49it/s]\n",
      "Shuffle Reduce:  82%|████████▎ | 165/200 [00:07<00:01, 30.38it/s]\n",
      "Shuffle Reduce:  94%|█████████▍| 188/200 [00:07<00:00, 43.63it/s]\n",
      "Shuffle Reduce:  99%|█████████▉| 198/200 [00:08<00:00, 22.03it/s]\n",
      "Shuffle Reduce: 100%|██████████| 200/200 [00:09<00:00, 21.51it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition:  12%|█▎        | 1/8 [00:05<00:41,  5.97s/it]\n",
      "Repartition:  25%|██▌       | 2/8 [00:06<00:15,  2.53s/it]\n",
      "Repartition:  62%|██████▎   | 5/8 [00:06<00:02,  1.29it/s]\n",
      "Repartition:  88%|████████▊ | 7/8 [00:06<00:00,  1.98it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:06<00:00,  1.19it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition:  12%|█▎        | 1/8 [00:02<00:16,  2.31s/it]\n",
      "Repartition:  25%|██▌       | 2/8 [00:02<00:06,  1.01s/it]\n",
      "Repartition:  62%|██████▎   | 5/8 [00:02<00:00,  3.01it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:02<00:00,  2.92it/s]\n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:25:13,839\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=5299, ip=10.0.0.49)\u001b[0m [11:25:19] task [xgboost.ray]:139948544085536 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3564, ip=10.0.0.204)\u001b[0m [11:25:19] task [xgboost.ray]:140037885963040 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3947, ip=10.0.0.152)\u001b[0m [11:25:19] task [xgboost.ray]:140310149614992 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3500, ip=10.0.0.187)\u001b[0m [11:25:19] task [xgboost.ray]:140020789969056 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=5216, ip=10.0.0.61)\u001b[0m [11:25:19] task [xgboost.ray]:140650302468208 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3625, ip=10.0.0.218)\u001b[0m [11:25:19] task [xgboost.ray]:139790166167408 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=7009, ip=10.0.0.85)\u001b[0m [11:25:19] task [xgboost.ray]:140327173319840 got new rank 7\n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:25:19,899\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3699, ip=10.0.0.32)\u001b[0m [11:25:19] task [xgboost.ray]:140294484037344 got new rank 4\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]947, ip=10.0.0.152)\u001b[0m \n",
      " 10%|█         | 1/10 [00:24<03:39, 24.34s/it]10.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:25:49,989\tINFO main.py:1099 -- Training in progress (30 seconds since last restart).\n",
      " 20%|██        | 2/10 [00:42<02:44, 20.57s/it]10.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:26:20,042\tINFO main.py:1099 -- Training in progress (60 seconds since last restart).\n",
      " 30%|███       | 3/10 [01:00<02:17, 19.62s/it]10.0.0.152)\u001b[0m \n",
      " 40%|████      | 4/10 [01:19<01:55, 19.27s/it]10.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:26:50,097\tINFO main.py:1099 -- Training in progress (90 seconds since last restart).\n",
      " 50%|█████     | 5/10 [01:37<01:34, 18.99s/it]10.0.0.152)\u001b[0m \n",
      " 60%|██████    | 6/10 [01:57<01:16, 19.00s/it]10.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:27:20,155\tINFO main.py:1099 -- Training in progress (120 seconds since last restart).\n",
      " 70%|███████   | 7/10 [02:15<00:56, 18.77s/it]10.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:27:50,208\tINFO main.py:1099 -- Training in progress (150 seconds since last restart).\n",
      " 80%|████████  | 8/10 [02:33<00:37, 18.72s/it]10.0.0.152)\u001b[0m \n",
      " 90%|█████████ | 9/10 [02:51<00:18, 18.46s/it]10.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:28:20,261\tINFO main.py:1099 -- Training in progress (180 seconds since last restart).\n",
      "100%|██████████| 10/10 [03:09<00:00, 18.97s/it]0.0.0.152)\u001b[0m \n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m 2022-01-26 11:28:32,118\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=21,000,000 in 198.36 seconds (192.21 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m Total time taken: 523.420551776886\n",
      "\u001b[2m\u001b[36m(train_xgboost_remote pid=45381)\u001b[0m Final validation error: 0.1488\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bst = ray.get(remote_train_xgboost_remote.remote(xgboost_params, s3_data_files_11GB, RayParams(num_actors=8, cpus_per_actor=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180283e",
   "metadata": {},
   "source": [
    "# 2. Hyperparameter Tuning\n",
    "\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) will launch distributed HPO, using 8 remote actors, each with its own instance of the trainable func: `train_xgboost` defined above.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dmatrix/ray-core-tutorial/main/images/ray_tune_dist_hpo.png\" width=\"40%\" height=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0332e",
   "metadata": {},
   "source": [
    "# Weight and Biases integration\n",
    "We just added the WandB callback to log all of our experiment metrics into WandB in the tune.run callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29b09cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:04:58,073\tWARNING function_runner.py:561 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:08:26,534\tWARNING trial_runner.py:276 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (149 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent trials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:26 (running for 00:00:00.39)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.1/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 17.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (7 PENDING, 1 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: WARNING Tried to auto resume run with id 8b1d5_00000 but id 8b1d5_00001 is set.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: WARNING Tried to auto resume run with id 8b1d5_00001 but id 8b1d5_00002 is set.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: WARNING Tried to auto resume run with id 8b1d5_00002 but id 8b1d5_00003 is set.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: WARNING Tried to auto resume run with id 8b1d5_00003 but id 8b1d5_00004 is set.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: WARNING Tried to auto resume run with id 8b1d5_00004 but id 8b1d5_00005 is set.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:08:28,514\tERROR gcs_utils.py:136 -- Failed to send request to gcs, reconnecting. Error <_InactiveRpcError of RPC that terminated with:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \tstatus = StatusCode.UNAVAILABLE\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \tdetails = \"Broken pipe\"\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \tdebug_error_string = \"{\"created\":\"@1643126908.513891502\",\"description\":\"Error received from peer ipv4:10.0.0.137:9031\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1074,\"grpc_message\":\"Broken pipe\",\"grpc_status\":14}\"\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m >\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:08:29,518\tWARNING util.py:165 -- The `start_trial` operation took 1.005 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:31 (running for 00:00:05.25)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.5/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m E0125 08:08:32.348522237   20921 chttp2_transport.cc:1103]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00000\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00000\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00000\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00002\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00002\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00002\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00001\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00001\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00001\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00003\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00003\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00003\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00004\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00004\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00004\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00005\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00005\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00005\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126917 on cpu 1 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:37,350 E 20887 20920] logging.cc:317: *** SIGSEGV received at time=1643126917 on cpu 1 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:37,351 E 20887 20920] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:37,351 E 20887 20920] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:37 (running for 00:00:11.17)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.9/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126922 on cpu 1 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:42,350 E 20810 20918] logging.cc:317: *** SIGSEGV received at time=1643126922 on cpu 1 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:42,350 E 20810 20918] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:42,351 E 20810 20918] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:42 (running for 00:00:16.18)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:47 (running for 00:00:21.20)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126932 on cpu 3 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:52,350 E 20902 20927] logging.cc:317: *** SIGSEGV received at time=1643126932 on cpu 3 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:52,351 E 20902 20927] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:08:52,351 E 20902 20927] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:52 (running for 00:00:26.22)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:08:57 (running for 00:00:31.33)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.1/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:02 (running for 00:00:36.35)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.8/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:  72%|███████▏  | 23/32 [00:00<00:00, 220.55it/s]\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 220.93it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Repartition:  31%|███▏      | 10/32 [00:00<00:00, 89.39it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 196.67it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  20%|██        | 2/10 [00:00<00:00, 13.04it/s]\n",
      "Repartition:  66%|██████▌   | 21/32 [00:00<00:00, 97.52it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:00,  9.93it/s]\n",
      "Shuffle Map:  40%|████      | 4/10 [00:00<00:00, 14.29it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 102.51it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  70%|███████   | 7/10 [00:00<00:00, 19.89it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:01,  8.60it/s]\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Shuffle Map:  30%|███       | 3/10 [00:00<00:00, 12.30it/s]\n",
      "Repartition:  25%|██▌       | 8/32 [00:00<00:00, 70.83it/s]\n",
      "Shuffle Map:  20%|██        | 2/10 [00:00<00:01,  7.35it/s]\n",
      "Repartition:  59%|█████▉    | 19/32 [00:00<00:00, 88.64it/s]\n",
      "Shuffle Map:  50%|█████     | 5/10 [00:00<00:00, 11.67it/s]\n",
      "Shuffle Map:  40%|████      | 4/10 [00:00<00:00, 10.85it/s]\n",
      "Repartition:  88%|████████▊ | 28/32 [00:00<00:00, 83.72it/s]\n",
      "Shuffle Map:  90%|█████████ | 9/10 [00:00<00:00, 19.34it/s]\n",
      "Shuffle Map:  60%|██████    | 6/10 [00:00<00:00, 12.39it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 78.32it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 11.76it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 14.77it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:00,  9.98it/s]\n",
      "Shuffle Reduce:  10%|█         | 1/10 [00:00<00:00,  9.72it/s]\n",
      "Shuffle Reduce:  10%|█         | 1/10 [00:00<00:01,  8.37it/s]\n",
      "Shuffle Map:  90%|█████████ | 9/10 [00:00<00:00, 13.42it/s]\n",
      "Shuffle Map:  20%|██        | 2/10 [00:00<00:00,  9.70it/s]\n",
      "Shuffle Reduce:  40%|████      | 4/10 [00:00<00:00, 18.66it/s]\n",
      "Shuffle Reduce:  40%|████      | 4/10 [00:00<00:00, 20.11it/s]\n",
      "Shuffle Reduce:  60%|██████    | 6/10 [00:00<00:00, 19.05it/s]\n",
      "Shuffle Map:  40%|████      | 4/10 [00:00<00:00, 11.38it/s]\n",
      "Shuffle Reduce:  70%|███████   | 7/10 [00:00<00:00, 15.93it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 10.31it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Reduce:  30%|███       | 3/10 [00:00<00:00, 27.53it/s]\n",
      "Shuffle Reduce:  80%|████████  | 8/10 [00:00<00:00, 11.05it/s]\n",
      "Shuffle Reduce:  60%|██████    | 6/10 [00:00<00:00, 26.23it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 14.41it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Reduce:  10%|█         | 1/10 [00:00<00:01,  8.58it/s]\n",
      "Shuffle Reduce:  90%|█████████ | 9/10 [00:00<00:00,  9.94it/s]\n",
      "Shuffle Reduce:  90%|█████████ | 9/10 [00:00<00:00, 19.29it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 10.84it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 10.93it/s]\n",
      "Shuffle Reduce:  40%|████      | 4/10 [00:00<00:00, 12.06it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 16.07it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 21.38it/s]\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126947 on cpu 5 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:07,350 E 20847 20849] logging.cc:317: *** SIGSEGV received at time=1643126947 on cpu 5 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:07,351 E 20847 20849] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:07,351 E 20847 20849] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition:  75%|███████▌  | 6/8 [00:00<00:00, 52.07it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition:  62%|██████▎   | 5/8 [00:00<00:00, 40.54it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 58.59it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 27.98it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 20.38it/s]\n",
      "Repartition:  38%|███▊      | 3/8 [00:00<00:00, 26.97it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 166.78it/s]\n",
      "Repartition:  75%|███████▌  | 6/8 [00:00<00:00, 26.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:07 (running for 00:00:41.37)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 23.95it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 127.21it/s]\n",
      "Repartition:  56%|█████▋    | 18/32 [00:00<00:00, 148.71it/s]\n",
      "Repartition:  88%|████████▊ | 7/8 [00:00<00:00, 66.97it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 134.46it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 39.36it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 109.83it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:01,  8.04it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=592, ip=10.0.0.176)\u001b[0m 2022-01-25 08:09:08,778\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "Shuffle Map:  30%|███       | 3/10 [00:00<00:00, 12.81it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=617, ip=10.0.0.85)\u001b[0m 2022-01-25 08:09:08,843\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 22.86it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=757, ip=10.0.0.247)\u001b[0m 2022-01-25 08:09:08,952\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=1090, ip=10.0.0.13)\u001b[0m 2022-01-25 08:09:09,129\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "Shuffle Reduce:  10%|█         | 1/10 [00:00<00:02,  3.80it/s]\n",
      "Shuffle Reduce:  40%|████      | 4/10 [00:00<00:00, 11.92it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=592, ip=10.0.0.176)\u001b[0m 2022-01-25 08:09:09,384\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=595, ip=10.0.0.166)\u001b[0m [08:09:09] task [xgboost.ray]:139612707876672 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=553, ip=10.0.0.247)\u001b[0m [08:09:09] task [xgboost.ray]:139843936770128 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=591, ip=10.0.0.14)\u001b[0m [08:09:09] task [xgboost.ray]:139640495222256 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=633, ip=10.0.0.166)\u001b[0m [08:09:09] task [xgboost.ray]:140301841711360 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=595, ip=10.0.0.14)\u001b[0m [08:09:09] task [xgboost.ray]:140353528534832 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=641, ip=10.0.0.247)\u001b[0m [08:09:09] task [xgboost.ray]:140180950692096 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=623, ip=10.0.0.14)\u001b[0m [08:09:09] task [xgboost.ray]:140443436260128 got new rank 1\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=757, ip=10.0.0.247)\u001b[0m 2022-01-25 08:09:09,412\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=805, ip=10.0.0.247)\u001b[0m [08:09:09] task [xgboost.ray]:140568409801104 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1457, ip=10.0.0.247)\u001b[0m [08:09:09] task [xgboost.ray]:140369846979696 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1564, ip=10.0.0.247)\u001b[0m [08:09:09] task [xgboost.ray]:140495677304016 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=590, ip=10.0.0.176)\u001b[0m [08:09:09] task [xgboost.ray]:140526985043296 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=736, ip=10.0.0.176)\u001b[0m [08:09:09] task [xgboost.ray]:140252078782784 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=742, ip=10.0.0.176)\u001b[0m [08:09:09] task [xgboost.ray]:139668999399120 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=746, ip=10.0.0.176)\u001b[0m [08:09:09] task [xgboost.ray]:140542647189616 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=919, ip=10.0.0.176)\u001b[0m [08:09:09] task [xgboost.ray]:140269747149408 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=912, ip=10.0.0.176)\u001b[0m [08:09:09] task [xgboost.ray]:140005292772560 got new rank 3\n",
      "Shuffle Reduce:  60%|██████    | 6/10 [00:00<00:00,  9.60it/s]\n",
      "Shuffle Reduce:  80%|████████  | 8/10 [00:00<00:00, 11.37it/s]\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=640, ip=10.0.0.13)\u001b[0m [08:09:09] task [xgboost.ray]:139616503111792 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=669, ip=10.0.0.13)\u001b[0m [08:09:09] task [xgboost.ray]:140088625598368 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=727, ip=10.0.0.13)\u001b[0m [08:09:09] task [xgboost.ray]:139898161954480 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=901, ip=10.0.0.13)\u001b[0m [08:09:09] task [xgboost.ray]:140695816997904 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=931, ip=10.0.0.13)\u001b[0m [08:09:09] task [xgboost.ray]:140465168276832 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=595, ip=10.0.0.65)\u001b[0m [08:09:09] task [xgboost.ray]:140240628499216 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=961, ip=10.0.0.13)\u001b[0m [08:09:09] task [xgboost.ray]:140536897698064 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=611, ip=10.0.0.65)\u001b[0m [08:09:09] task [xgboost.ray]:140404352665296 got new rank 7\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=1090, ip=10.0.0.13)\u001b[0m 2022-01-25 08:09:09,827\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 10.90it/s]\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=598, ip=10.0.0.85)\u001b[0m [08:09:09] task [xgboost.ray]:140124326874656 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=600, ip=10.0.0.85)\u001b[0m [08:09:09] task [xgboost.ray]:140559102607264 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=603, ip=10.0.0.85)\u001b[0m [08:09:09] task [xgboost.ray]:140577442167920 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=610, ip=10.0.0.85)\u001b[0m [08:09:09] task [xgboost.ray]:140546749942608 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=615, ip=10.0.0.85)\u001b[0m [08:09:09] task [xgboost.ray]:139969624957808 got new rank 5\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=617, ip=10.0.0.85)\u001b[0m 2022-01-25 08:09:09,933\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=620, ip=10.0.0.85)\u001b[0m [08:09:09] task [xgboost.ray]:140025520442288 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=725, ip=10.0.0.199)\u001b[0m [08:09:09] task [xgboost.ray]:140279828131168 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=960, ip=10.0.0.199)\u001b[0m [08:09:09] task [xgboost.ray]:140351975063760 got new rank 1\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 44.37it/s]\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_shuffle_reduce pid=637, ip=10.0.0.199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:10,700\tINFO commands.py:292 -- Checking External environment settings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition:  72%|███████▏  | 23/32 [00:00<00:00, 204.77it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 208.82it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:01,  7.54it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 42.92it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 186.28it/s]\n",
      "Shuffle Reduce:  10%|█         | 1/10 [00:00<00:01,  8.10it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 34.71it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=608, ip=10.0.0.199)\u001b[0m 2022-01-25 08:09:11,399\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=597, ip=10.0.0.166)\u001b[0m [08:09:11] task [xgboost.ray]:140314864567776 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=658, ip=10.0.0.166)\u001b[0m [08:09:11] task [xgboost.ray]:139777001371104 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=642, ip=10.0.0.166)\u001b[0m [08:09:11] task [xgboost.ray]:140599575284560 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=660, ip=10.0.0.166)\u001b[0m [08:09:11] task [xgboost.ray]:140641370262016 got new rank 3\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=608, ip=10.0.0.199)\u001b[0m 2022-01-25 08:09:11,730\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1081, ip=10.0.0.199)\u001b[0m [08:09:11] task [xgboost.ray]:140104316996912 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1111, ip=10.0.0.199)\u001b[0m [08:09:11] task [xgboost.ray]:140298486174144 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1235, ip=10.0.0.199)\u001b[0m [08:09:11] task [xgboost.ray]:140437694266720 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3114, ip=10.0.0.199)\u001b[0m [08:09:11] task [xgboost.ray]:139919545789600 got new rank 5\n",
      "Repartition:  88%|████████▊ | 7/8 [00:00<00:00, 69.17it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 48.11it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 89.24it/s]\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126952 on cpu 3 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x564957f25e71  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:12,355 E 20822 20925] logging.cc:317: *** SIGSEGV received at time=1643126952 on cpu 3 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:12,358 E 20822 20925] logging.cc:317: PC: @     0x564957f25e71  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:12,360 E 20822 20925] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:12,595\tWARN util.py:141 -- The `worker_nodes` field is deprecated and will be ignored. Use `available_node_types` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=874, ip=10.0.0.65)\u001b[0m 2022-01-25 08:09:12,697\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \u001b[1m\u001b[36mAuthenticating\u001b[0m\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Loaded Anyscale authentication token from variable.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=594, ip=10.0.0.14)\u001b[0m [08:09:15] task [xgboost.ray]:140375843993296 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=603, ip=10.0.0.14)\u001b[0m [08:09:15] task [xgboost.ray]:140059199810048 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=597, ip=10.0.0.14)\u001b[0m [08:09:15] task [xgboost.ray]:140276367636896 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=6246)\u001b[0m [08:09:15] task [xgboost.ray]:140140036855168 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=7286)\u001b[0m [08:09:15] task [xgboost.ray]:140386444357104 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=602, ip=10.0.0.65)\u001b[0m [08:09:15] task [xgboost.ray]:139639722249040 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=646, ip=10.0.0.65)\u001b[0m [08:09:15] task [xgboost.ray]:139738108487088 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=643, ip=10.0.0.65)\u001b[0m [08:09:15] task [xgboost.ray]:139870442774688 got new rank 6\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=874, ip=10.0.0.65)\u001b[0m 2022-01-25 08:09:15,070\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:16,901\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.176\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:16,901\tINFO log_timer.py:25 -- NodeUpdater: ins_jWYB6jsj1QzcDXgxiShXUb5Y: Got IP  [LogTimer=576ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126957 on cpu 7 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:17,351 E 20860 20926] logging.cc:317: *** SIGSEGV received at time=1643126957 on cpu 7 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:17,352 E 20860 20926] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:17,352 E 20860 20926] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:18 (running for 00:00:51.79)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.0/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00004 with eval-error=0.30766 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0020346618470171484, 'subsample': 0.7909773018031685, 'max_depth': 3, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:18,395\tWARN commands.py:269 -- Loaded cached provider configuration\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:18,395\tWARN commands.py:270 -- If you experience issues with the cloud provider, try re-running the command with --no-config-cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:18,313\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 7.876 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:18,313\tWARNING util.py:165 -- The `process_trial_result` operation took 7.877 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:18,313\tWARNING util.py:165 -- Processing trial results took 7.877 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:18,313\tWARNING util.py:165 -- The `process_trial` operation took 7.877 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:20,860\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.65\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:20,860\tINFO log_timer.py:25 -- NodeUpdater: ins_1K7MdWdebpK4i77CsekEKiLW: Got IP  [LogTimer=105ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:21,827\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 3.492 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:21,828\tWARNING util.py:165 -- The `process_trial_result` operation took 3.493 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:21,828\tWARNING util.py:165 -- Processing trial results took 3.493 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:21,828\tWARNING util.py:165 -- The `process_trial` operation took 3.493 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:24,322\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.199\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:24,322\tINFO log_timer.py:25 -- NodeUpdater: ins_2zfRTDth5fbbiYXk2tDPi2ru: Got IP  [LogTimer=86ms]\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:25 (running for 00:00:58.84)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.0/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00000 with eval-error=0.24240699999999998 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0036490762865544015, 'subsample': 0.5594855338871798, 'max_depth': 6, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:25,374\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 3.542 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:25,375\tWARNING util.py:165 -- The `process_trial_result` operation took 3.543 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:25,375\tWARNING util.py:165 -- Processing trial results took 3.543 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:25,375\tWARNING util.py:165 -- The `process_trial` operation took 3.543 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:28,590\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.13\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:28,590\tINFO log_timer.py:25 -- NodeUpdater: ins_ac6zgMX3nvXgY9XpRJgDwwuh: Got IP  [LogTimer=531ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:29,634\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 4.253 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:29,634\tWARNING util.py:165 -- The `process_trial_result` operation took 4.254 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:29,634\tWARNING util.py:165 -- Processing trial results took 4.254 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:29,634\tWARNING util.py:165 -- The `process_trial` operation took 4.254 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126972 on cpu 3 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:32,350 E 20773 20917] logging.cc:317: *** SIGSEGV received at time=1643126972 on cpu 3 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:32,350 E 20773 20917] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:32,350 E 20773 20917] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:33,002\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.85\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:33,003\tINFO log_timer.py:25 -- NodeUpdater: ins_U8mQnSeXzt6f6GwrquWfd36S: Got IP  [LogTimer=208ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:34,061\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 4.402 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:34,062\tWARNING util.py:165 -- The `process_trial_result` operation took 4.403 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:34,062\tWARNING util.py:165 -- Processing trial results took 4.403 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:34,062\tWARNING util.py:165 -- The `process_trial` operation took 4.403 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:34 (running for 00:01:07.53)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.1/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 102.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00000 with eval-error=0.24240699999999998 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0036490762865544015, 'subsample': 0.5594855338871798, 'max_depth': 6, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 6 RUNNING)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:36,517\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.247\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:36,517\tINFO log_timer.py:25 -- NodeUpdater: ins_w4xZksByUZcYpNtJ6zNQMs5t: Got IP  [LogTimer=41ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126977 on cpu 5 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:37,350 E 20801 20922] logging.cc:317: *** SIGSEGV received at time=1643126977 on cpu 5 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:37,350 E 20801 20922] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:37,350 E 20801 20922] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:37,519\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 3.452 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:37,520\tWARNING util.py:165 -- The `process_trial_result` operation took 3.453 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:37,520\tWARNING util.py:165 -- Processing trial results took 3.453 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:37,520\tWARNING util.py:165 -- The `process_trial` operation took 3.453 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=1090, ip=10.0.0.13)\u001b[0m 2022-01-25 08:09:37,645\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 28.61 seconds (27.81 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=1090, ip=10.0.0.13)\u001b[0m Total time taken: 70.41572999954224\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=1090, ip=10.0.0.13)\u001b[0m Final validation error: 0.2861\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Waiting for W&B process to finish, PID 20902... (success).\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126982 on cpu 1 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:42,350 E 20784 20848] logging.cc:317: *** SIGSEGV received at time=1643126982 on cpu 1 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:42,350 E 20784 20848] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:42,350 E 20784 20848] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:47 (running for 00:01:21.19)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.0/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 85.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00001 with eval-error=0.17505 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0002752208184848541, 'subsample': 0.5922580193607261, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 PENDING, 5 RUNNING, 1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=592, ip=10.0.0.176)\u001b[0m Total time taken: 80.66854929924011\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=592, ip=10.0.0.176)\u001b[0m Final validation error: 0.2866\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=757, ip=10.0.0.247)\u001b[0m Total time taken: 80.72682237625122\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=757, ip=10.0.0.247)\u001b[0m Final validation error: 0.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:47,717\tWARNING util.py:165 -- The `process_trial_result` operation took 10.037 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:47,717\tWARNING util.py:165 -- Processing trial results took 10.037 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:47,717\tWARNING util.py:165 -- The `process_trial` operation took 10.037 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=757, ip=10.0.0.247)\u001b[0m 2022-01-25 08:09:47,813\tINFO main.py:1099 -- Training in progress (38 seconds since last restart).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=592, ip=10.0.0.176)\u001b[0m 2022-01-25 08:09:47,821\tINFO main.py:1099 -- Training in progress (38 seconds since last restart).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=592, ip=10.0.0.176)\u001b[0m 2022-01-25 08:09:47,827\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 39.09 seconds (38.43 pure XGBoost training time).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=757, ip=10.0.0.247)\u001b[0m 2022-01-25 08:09:47,837\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 38.98 seconds (38.41 pure XGBoost training time).\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643126992 on cpu 0 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:52,351 E 20832 20915] logging.cc:317: *** SIGSEGV received at time=1643126992 on cpu 0 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:09:52,351 E 20832 20915] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00006\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00006\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080947-8b1d5_00006\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:57,878\tWARNING util.py:165 -- The `process_trial_result` operation took 10.012 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:57,878\tWARNING util.py:165 -- Processing trial results took 10.012 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:09:57,878\tWARNING util.py:165 -- The `process_trial` operation took 10.012 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:09:57 (running for 00:01:31.38)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.3/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 85.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00001 with eval-error=0.173947 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0002752208184848541, 'subsample': 0.5922580193607261, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (1 PENDING, 5 RUNNING, 2 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Currently logged in as: sublimotion (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: WARNING Tried to auto resume run with id 8b1d5_00006 but id 8b1d5_00007 is set.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Tracking run with wandb version 0.12.5\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Syncing run train_xgboost_remote_8b1d5_00007\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View project at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:  View run at https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00007\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080958-8b1d5_00007\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:07 (running for 00:01:41.45)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 85.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00001 with eval-error=0.173947 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0002752208184848541, 'subsample': 0.5922580193607261, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (5 RUNNING, 3 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=608, ip=10.0.0.199)\u001b[0m Total time taken: 100.88346862792969\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=608, ip=10.0.0.199)\u001b[0m Final validation error: 0.2305\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=617, ip=10.0.0.85)\u001b[0m Total time taken: 100.83817791938782\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=617, ip=10.0.0.85)\u001b[0m Final validation error: 0.3778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:07,974\tWARNING util.py:165 -- The `process_trial_result` operation took 10.012 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:07,975\tWARNING util.py:165 -- Processing trial results took 10.013 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:07,975\tWARNING util.py:165 -- The `process_trial` operation took 10.014 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=617, ip=10.0.0.85)\u001b[0m 2022-01-25 08:10:08,001\tINFO main.py:1099 -- Training in progress (58 seconds since last restart).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=617, ip=10.0.0.85)\u001b[0m 2022-01-25 08:10:08,006\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 59.21 seconds (58.06 pure XGBoost training time).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=608, ip=10.0.0.199)\u001b[0m 2022-01-25 08:10:07,991\tINFO main.py:1099 -- Training in progress (56 seconds since last restart).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=608, ip=10.0.0.199)\u001b[0m 2022-01-25 08:10:07,998\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 56.66 seconds (56.26 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Waiting for W&B process to finish, PID 20860... (success).\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:  69%|██████▉   | 22/32 [00:00<00:00, 208.69it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 203.09it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:01,  8.42it/s]\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:18,040\tWARNING util.py:165 -- The `process_trial_result` operation took 10.013 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:18,042\tWARNING util.py:165 -- Processing trial results took 10.015 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:18,042\tWARNING util.py:165 -- The `process_trial` operation took 10.016 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:18 (running for 00:01:51.56)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 68.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00001 with eval-error=0.173947 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0002752208184848541, 'subsample': 0.5922580193607261, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (4 RUNNING, 4 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map:  50%|█████     | 5/10 [00:00<00:00, 12.85it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 19.13it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Reduce:  10%|█         | 1/10 [00:00<00:01,  6.05it/s]\n",
      "Shuffle Reduce:  60%|██████    | 6/10 [00:00<00:00, 24.76it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:02<00:00,  4.28it/s]\n",
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Repartition:  38%|███▊      | 3/8 [00:00<00:00, 29.94it/s]\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m *** SIGSEGV received at time=1643127022 on cpu 0 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:10:22,350 E 20792 20924] logging.cc:317: *** SIGSEGV received at time=1643127022 on cpu 0 ***\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:10:22,350 E 20792 20924] logging.cc:317: PC: @     0x7ff2ae3430a1  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m [2022-01-25 08:10:22,350 E 20792 20924] logging.cc:317:     @     0x7ff2b647b980  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Fatal Python error: Segmentation fault\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "Repartition:  75%|███████▌  | 6/8 [00:02<00:00,  2.39it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:02<00:00,  3.65it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 251.35it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=610, ip=10.0.0.13)\u001b[0m 2022-01-25 08:10:23,564\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=552, ip=10.0.0.13)\u001b[0m [08:10:26] task [xgboost.ray]:139867502137056 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=581, ip=10.0.0.13)\u001b[0m [08:10:26] task [xgboost.ray]:140553749451104 got new rank 0\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=610, ip=10.0.0.13)\u001b[0m 2022-01-25 08:10:26,162\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=756, ip=10.0.0.13)\u001b[0m [08:10:26] task [xgboost.ray]:140403525227808 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=991, ip=10.0.0.13)\u001b[0m [08:10:26] task [xgboost.ray]:139711104060096 got new rank 4\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1021, ip=10.0.0.13)\u001b[0m [08:10:26] task [xgboost.ray]:140352820877008 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=3976, ip=10.0.0.13)\u001b[0m [08:10:26] task [xgboost.ray]:139607603685696 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=591, ip=10.0.0.65)\u001b[0m [08:10:26] task [xgboost.ray]:140392848412048 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=593, ip=10.0.0.65)\u001b[0m [08:10:26] task [xgboost.ray]:140198305746896 got new rank 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:28 (running for 00:02:01.60)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.5/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 51.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00001 with eval-error=0.173947 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.0002752208184848541, 'subsample': 0.5922580193607261, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (3 RUNNING, 5 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:28,120\tWARNING util.py:165 -- The `process_trial_result` operation took 10.012 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:28,120\tWARNING util.py:165 -- Processing trial results took 10.012 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:28,120\tWARNING util.py:165 -- The `process_trial` operation took 10.012 s, which may be a performance bottleneck.\n",
      "Repartition:   0%|          | 0/32 [00:00<?, ?it/s]\n",
      "Repartition:  41%|████      | 13/32 [00:00<00:00, 123.71it/s]\n",
      "Repartition: 100%|██████████| 32/32 [00:00<00:00, 140.71it/s]\n",
      "Shuffle Map:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Map:  10%|█         | 1/10 [00:00<00:01,  8.20it/s]\n",
      "Shuffle Map:  70%|███████   | 7/10 [00:00<00:00, 32.85it/s]\n",
      "Shuffle Map: 100%|██████████| 10/10 [00:00<00:00, 37.97it/s]\n",
      "Shuffle Reduce:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Shuffle Reduce: 100%|██████████| 10/10 [00:00<00:00, 54.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:31,466\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.13\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:31,466\tINFO log_timer.py:25 -- NodeUpdater: ins_ac6zgMX3nvXgY9XpRJgDwwuh: Got IP  [LogTimer=483ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repartition:   0%|          | 0/8 [00:00<?, ?it/s] \n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 100.01it/s]\n",
      "Repartition: 100%|██████████| 8/8 [00:00<00:00, 199.78it/s]\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=725, ip=10.0.0.247)\u001b[0m 2022-01-25 08:10:32,056\tINFO main.py:979 -- [RayXGBoost] Created 8 new actors (8 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:32,292\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 4.145 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:32,292\tWARNING util.py:165 -- The `process_trial_result` operation took 4.146 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:32,293\tWARNING util.py:165 -- Processing trial results took 4.146 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:32,293\tWARNING util.py:165 -- The `process_trial` operation took 4.146 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=874, ip=10.0.0.65)\u001b[0m 2022-01-25 08:10:32,288\tINFO main.py:1099 -- Training in progress (77 seconds since last restart).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=874, ip=10.0.0.65)\u001b[0m 2022-01-25 08:10:32,298\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 79.66 seconds (77.22 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=874, ip=10.0.0.65)\u001b[0m Total time taken: 125.38759446144104\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=874, ip=10.0.0.65)\u001b[0m Final validation error: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=725, ip=10.0.0.247)\u001b[0m 2022-01-25 08:10:32,591\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=593, ip=10.0.0.14)\u001b[0m [08:10:32] task [xgboost.ray]:139967660168720 got new rank 1\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=617, ip=10.0.0.14)\u001b[0m [08:10:32] task [xgboost.ray]:139946725706384 got new rank 2\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=624, ip=10.0.0.14)\u001b[0m [08:10:32] task [xgboost.ray]:140694627156176 got new rank 0\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=612, ip=10.0.0.247)\u001b[0m [08:10:32] task [xgboost.ray]:140475920207152 got new rank 3\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=834, ip=10.0.0.247)\u001b[0m [08:10:32] task [xgboost.ray]:140587667979328 got new rank 5\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=864, ip=10.0.0.247)\u001b[0m [08:10:32] task [xgboost.ray]:139803864056880 got new rank 6\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=894, ip=10.0.0.247)\u001b[0m [08:10:32] task [xgboost.ray]:139988594528160 got new rank 7\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=1561, ip=10.0.0.247)\u001b[0m [08:10:32] task [xgboost.ray]:139928992649072 got new rank 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:42 (running for 00:02:15.83)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 13.1/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 34.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00006 with eval-error=0.17334 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.04340709367317894, 'subsample': 0.8957024019504654, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (2 RUNNING, 6 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:42,338\tWARNING util.py:165 -- The `process_trial_result` operation took 10.010 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:42,338\tWARNING util.py:165 -- Processing trial results took 10.010 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:42,338\tWARNING util.py:165 -- The `process_trial` operation took 10.011 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:45,913\tINFO command_runner.py:357 -- Fetched IP: 10.0.0.247\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:45,913\tINFO log_timer.py:25 -- NodeUpdater: ins_w4xZksByUZcYpNtJ6zNQMs5t: Got IP  [LogTimer=568ms]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:46,723\tWARNING util.py:165 -- The `callbacks.on_trial_result` operation took 4.347 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:46,724\tWARNING util.py:165 -- The `process_trial_result` operation took 4.347 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:46,724\tWARNING util.py:165 -- Processing trial results took 4.348 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:46,724\tWARNING util.py:165 -- The `process_trial` operation took 4.348 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=610, ip=10.0.0.13)\u001b[0m 2022-01-25 08:10:46,792\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 23.26 seconds (20.62 pure XGBoost training time).\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=725, ip=10.0.0.247)\u001b[0m 2022-01-25 08:10:46,787\tINFO main.py:1503 -- [RayXGBoost] Finished XGBoost training on training data with total N=700,000 in 14.77 seconds (14.19 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ImplicitFunc pid=610, ip=10.0.0.13)\u001b[0m Total time taken: 58.88376712799072\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=610, ip=10.0.0.13)\u001b[0m Final validation error: 0.1610\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=725, ip=10.0.0.247)\u001b[0m Total time taken: 48.71080565452576\n",
      "\u001b[2m\u001b[36m(ImplicitFunc pid=725, ip=10.0.0.247)\u001b[0m Final validation error: 0.3317\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Waiting for W&B process to finish, PID 22524... (success).\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \\ 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: | 0.00MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: / 0.00MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \\ 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \\ 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \\ 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:52 (running for 00:02:26.31)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.9/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 17.0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00006 with eval-error=0.16095 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.04340709367317894, 'subsample': 0.8957024019504654, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (1 RUNNING, 7 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 config/eta ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/max_depth ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:              config/n_jobs ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:             config/nthread ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/subsample ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 eval-error █▂▇▆▅▅▅▅▆▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               eval-logloss ████▇▇▇▃▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:   iterations_since_restore ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         time_since_restore ▁█████████\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           time_this_iter_s █▄▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               time_total_s ▁█████████\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                  timestamp ▁█████████\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:    timesteps_since_restore ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         training_iteration ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 config/eta 0.0001\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/max_depth 2\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:              config/n_jobs 2\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:             config/nthread 2\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/subsample 0.64722\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 eval-error 0.33171\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               eval-logloss 0.69295\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:   iterations_since_restore 10\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         time_since_restore 48.69466\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           time_this_iter_s 0.00551\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               time_total_s 48.69466\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                  timestamp 1643127046\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:    timesteps_since_restore 0\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         training_iteration 10\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Synced train_xgboost_remote_8b1d5_00007: https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00007\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Find logs at: ./wandb/run-20220125_080958-8b1d5_00007/logs/debug.log\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:52,797\tWARNING util.py:165 -- The `process_trial_result` operation took 5.990 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:52,797\tWARNING util.py:165 -- Processing trial results took 5.990 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:52,798\tWARNING util.py:165 -- The `process_trial` operation took 5.990 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Waiting for W&B process to finish, PID 22358... (success).\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \\ 0.00MB of 0.00MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: | 0.00MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: / 0.00MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \\ 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:58 (running for 00:02:31.86)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00006 with eval-error=0.16095 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.04340709367317894, 'subsample': 0.8957024019504654, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (8 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current time: 2022-01-25 08:10:58 (running for 00:02:31.91)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Memory usage on this node: 12.6/30.9 GiB\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Resources requested: 0/136 CPUs, 0/0 GPUs, 0.0/376.72 GiB heap, 0.0/156.92 GiB objects\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Current best trial: 8b1d5_00006 with eval-error=0.16095 and parameters={'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.04340709367317894, 'subsample': 0.8957024019504654, 'max_depth': 8, 'nthread': 2, 'n_jobs': 2}\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Result logdir: /home/ray/ray_results/train_xgboost_remote_2022-01-25_08-04-58\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Number of trials: 8/8 (8 TERMINATED)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 config/eta ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/max_depth ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:              config/n_jobs ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:             config/nthread ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/subsample ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 eval-error █▅▅▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               eval-logloss █▇▆▅▄▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:   iterations_since_restore ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         time_since_restore ▁▃▃▃▃▃████\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           time_this_iter_s █▂▁▁▁▁▄▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               time_total_s ▁▃▃▃▃▃████\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                  timestamp ▁▃▃▃▃▃████\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:    timesteps_since_restore ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         training_iteration ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 config/eta 0.04341\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/max_depth 8\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:              config/n_jobs 2\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:             config/nthread 2\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           config/subsample 0.8957\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                 eval-error 0.16095\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               eval-logloss 0.56761\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:   iterations_since_restore 10\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         time_since_restore 58.84202\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:           time_this_iter_s 0.00727\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:               time_total_s 58.84202\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:                  timestamp 1643127046\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:    timesteps_since_restore 0\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb:         training_iteration 10\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Synced train_xgboost_remote_8b1d5_00006: https://wandb.ai/sublimotion/XGBoost-Tune-Experiment-demo/runs/8b1d5_00006\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: Find logs at: ./wandb/run-20220125_080947-8b1d5_00006/logs/debug.log\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:58,395\tWARNING util.py:165 -- The `process_trial_result` operation took 5.540 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:58,396\tWARNING util.py:165 -- Processing trial results took 5.540 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:58,396\tWARNING util.py:165 -- The `process_trial` operation took 5.540 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m 2022-01-25 08:10:58,558\tINFO tune.py:626 -- Total run time: 360.49 seconds (151.86 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.04340709367317894, 'subsample': 0.8957024019504654, 'max_depth': 8}\n",
      "Best model total accuracy: 0.8390\n",
      "{'tree_method': 'approx', 'objective': 'binary:logistic', 'eval_metric': ['logloss', 'error'], 'eta': 0.04340709367317894, 'subsample': 0.8957024019504654, 'max_depth': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m --- Logging error ---\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self.flush()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/logging/__init__.py\", line 1065, in flush\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self.stream.flush()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m OSError: [Errno 28] No space left on device\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Call stack:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/threading.py\", line 890, in _bootstrap\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._bootstrap_inner()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 52, in run\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._run()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 103, in _run\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._finish()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/internal.py\", line 295, in _finish\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._sm.finish()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 1033, in finish\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._dir_watcher.finish()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/filesync/dir_watcher.py\", line 327, in finish\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Message: 'scan save: %s %s'\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Arguments: ('/tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00003/files/wandb-metadata.json', 'wandb-metadata.json')\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m --- Logging error ---\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self.flush()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/logging/__init__.py\", line 1065, in flush\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self.stream.flush()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m OSError: [Errno 28] No space left on device\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Call stack:\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/threading.py\", line 890, in _bootstrap\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._bootstrap_inner()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 52, in run\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._run()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 103, in _run\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._finish()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/internal.py\", line 295, in _finish\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._sm.finish()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 1033, in finish\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     self._dir_watcher.finish()\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/wandb/filesync/dir_watcher.py\", line 327, in finish\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m     logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Message: 'scan save: %s %s'\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Arguments: ('/tmp/ray/session_2022-01-25_07-11-21_216444_163/runtime_resources/working_dir_files/_ray_pkg_226d4ea0735c6585/wandb/run-20220125_080827-8b1d5_00003/files/config.yaml', 'config.yaml')\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m --- Logging error ---\n",
      "\u001b[2m\u001b[36m(run pid=5676)\u001b[0m Tracebac\n",
      "Log channel is reconnecting. Logs produced while the connection was down can be found on the head node of the cluster in `ray_client_server_[port].out`\n",
      "2022-01-25 17:32:27,444\tWARNING dataclient.py:220 -- Encountered connection issues in the data channel. Attempting to reconnect.\n",
      "2022-01-25 17:32:58,138\tWARNING dataclient.py:226 -- Failed to reconnect the data channel\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "import os\n",
    "from ray.tune.integration.wandb import WandbLoggerCallback\n",
    "\n",
    "project_name = f\"XGBoost-Tune-Experiment-demo\"\n",
    "api_key = \"xxxxxxxxxxxx\" # TODO: change this if you have your own API key\n",
    "os.environ[\"TUNE_DISABLE_AUTO_CALLBACK_LOGGERS\"] = \"1\"\n",
    "\n",
    "# Set XGBoost config.\n",
    "config = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"subsample\": tune.uniform(0.5, 1.0),\n",
    "    \"max_depth\": tune.randint(1, 9)\n",
    "}\n",
    "\n",
    "ray_params = RayParams(\n",
    "    max_actor_restarts=1,\n",
    "    gpus_per_actor=0,\n",
    "    cpus_per_actor=2,\n",
    "    num_actors=8)\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune.with_parameters(train_xgboost_remote, files=s3_data_files_300MB, ray_params=ray_params, progress_bar=False),\n",
    "    # Use the `get_tune_resources` helper function to set the resources.\n",
    "    resources_per_trial=ray_params.get_tune_resources(),\n",
    "    config=config,\n",
    "    num_samples=8,\n",
    "    metric=\"eval-error\",\n",
    "    mode=\"min\",\n",
    "    verbose=1,\n",
    "    callbacks=[WandbLoggerCallback(\n",
    "            project= project_name,\n",
    "            api_key= api_key,\n",
    "            log_config=True)]\n",
    "    )\n",
    "\n",
    "accuracy = 1. - analysis.best_result[\"eval-error\"]\n",
    "print(f\"Best model parameters: {analysis.best_config}\")\n",
    "print(f\"Best model total accuracy: {accuracy:.4f}\")\n",
    "print(analysis.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec8f07",
   "metadata": {},
   "source": [
    "# 3a. Inference (regular XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5810270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 0.337000372 GB\n",
      "Total time taken: 1.5613970756530762\n"
     ]
    }
   ],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "df = load_parquet_dataset(data_files_300MB).drop([\"labels\", \"partition\"], axis=1, errors=\"ignore\")\n",
    "inference_df = DMatrix(df)\n",
    "results = bst.predict(inference_df)\n",
    "\n",
    "print(f\"Total time taken: {time.time()-total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e62dce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7038215 , 0.18560845, 0.17597654, ..., 0.18545696, 0.87184376,\n",
       "       0.30593213], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de31b0",
   "metadata": {},
   "source": [
    "# 3b. Inference (XGBoost on Ray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e925ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def xgb_batch_inference(bst, files):\n",
    "    total_time = time.time()\n",
    "\n",
    "    inference_df = RayDMatrix(files, ignore=[\"labels\", \"partition\"])\n",
    "    results = xgboost_ray.predict(bst, inference_df, ray_params=RayParams(num_actors=8))\n",
    "\n",
    "    print(f\"Total time taken: {time.time()-total_time}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd4f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(xgb_batch_inference pid=46848)\u001b[0m 2022-01-26 11:30:40,396\tINFO main.py:1543 -- [RayXGBoost] Created 8 remote actors.\n",
      "\u001b[2m\u001b[36m(xgb_batch_inference pid=46848)\u001b[0m 2022-01-26 11:30:53,481\tINFO main.py:1560 -- [RayXGBoost] Starting XGBoost prediction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(xgb_batch_inference pid=46848)\u001b[0m Total time taken: 13.341913938522339\n"
     ]
    }
   ],
   "source": [
    "batch_results = ray.get(xgb_batch_inference.remote(bst, s3_data_files_300MB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf2cee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6322733 , 0.70809305, 0.36546   , ..., 0.17210634, 0.90754145,\n",
       "       0.25357166], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1d8ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e06090",
   "metadata": {},
   "source": [
    "# References\n",
    " * [Introducing Distributed XGBoost Training with Ray](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)\n",
    " * [How to Speed Up XGBoost Model Training](https://www.anyscale.com/blog/how-to-speed-up-xgboost-model-training)\n",
    " * [Distributed XGBoost on Ray](https://docs.ray.io/en/latest/xgboost-ray.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
